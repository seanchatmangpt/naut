#!/usr/bin/env python3
"""
Optimized Order Book Benchmark - Finding the Sweet Spot
"""
import time
import numpy as np
import subprocess
import ctypes
import os
import tempfile

OPTIMIZED_ASM = """
.section __TEXT,__text
.global _fast_sequential_update
.global _fast_random_update
.global _fast_hot_update
.align 4

// Sequential updates - best case scenario
// x0 = base_ptr, x1 = start_offset, x2 = count, x3 = value
_fast_sequential_update:
    add x0, x0, x1, lsl #3      // Calculate start address
    
seq_loop:
    str x3, [x0], #8            // Store and post-increment
    subs x2, x2, #1
    b.ne seq_loop
    
    ret

// Random updates with prefetch
// x0 = base_ptr, x1 = offsets_ptr, x2 = values_ptr, x3 = count
_fast_random_update:
    cbz x3, done_random
    
random_loop:
    ldr x4, [x1], #8            // Load offset
    ldr x5, [x2], #8            // Load value
    
    // Prefetch next offset
    prfm pldl1keep, [x1]
    
    str x5, [x0, x4, lsl #3]    // Store value
    
    subs x3, x3, #1
    b.ne random_loop
    
done_random:
    ret

// Hot path update - minimal overhead
// x0 = base_ptr, x1 = hot_offset, x2 = value
_fast_hot_update:
    str x2, [x0, x1, lsl #3]
    ret
"""

def run_optimized_benchmarks():
    """Test different update patterns"""
    print("üî• Optimized Order Book Benchmarks")
    print("=" * 60)
    
    with tempfile.TemporaryDirectory() as tmpdir:
        # Write and compile assembly
        asm_path = os.path.join(tmpdir, "optimized.s")
        with open(asm_path, 'w') as f:
            f.write(OPTIMIZED_ASM)
        
        obj_path = os.path.join(tmpdir, "optimized.o")
        result = subprocess.run([
            "clang", "-c", "-arch", "arm64", "-O3",
            asm_path, "-o", obj_path
        ], capture_output=True)
        
        if result.returncode != 0:
            print(f"‚ùå Compilation failed")
            return
        
        lib_path = os.path.join(tmpdir, "liboptimized.dylib")
        subprocess.run([
            "clang", "-shared", "-arch", "arm64",
            obj_path, "-o", lib_path
        ], capture_output=True)
        
        lib = ctypes.CDLL(lib_path)
        
        # Allocate order book
        book_size = 1024 * 1024 * 8  # 8MB
        book = (ctypes.c_byte * book_size)()
        book_ptr = ctypes.addressof(book)
        
        # Test 1: Sequential updates (best case)
        print("\nüìä Sequential Updates (Cache-Friendly)")
        print("-" * 50)
        
        lib.fast_sequential_update.argtypes = [
            ctypes.c_void_p, ctypes.c_uint64, 
            ctypes.c_uint64, ctypes.c_uint64
        ]
        
        count = 10_000_000
        start = time.perf_counter_ns()
        
        lib.fast_sequential_update(book_ptr, 0, count, 12345)
        
        end = time.perf_counter_ns()
        
        seq_ns = (end - start) / count
        print(f"  Updates: {count:,}")
        print(f"  Per update: {seq_ns:.3f}ns")
        print(f"  Throughput: {1e9/seq_ns:,.0f} updates/sec")
        
        # Test 2: Hot path (single location)
        print("\nüìä Hot Path Update (Single Location)")
        print("-" * 50)
        
        lib.fast_hot_update.argtypes = [
            ctypes.c_void_p, ctypes.c_uint64, ctypes.c_uint64
        ]
        
        iterations = 100_000_000
        hot_offset = 100  # Fixed location
        
        start = time.perf_counter_ns()
        
        for i in range(iterations):
            lib.fast_hot_update(book_ptr, hot_offset, i)
        
        end = time.perf_counter_ns()
        
        hot_ns = (end - start) / iterations
        print(f"  Updates: {iterations:,}")
        print(f"  Per update: {hot_ns:.3f}ns")
        print(f"  Throughput: {1e9/hot_ns:,.0f} updates/sec")
        
        # Test 3: Random updates with different ranges
        print("\nüìä Random Updates (Various Cache Levels)")
        print("-" * 50)
        
        lib.fast_random_update.argtypes = [
            ctypes.c_void_p, ctypes.c_void_p,
            ctypes.c_void_p, ctypes.c_uint64
        ]
        
        test_sizes = [
            (4 * 1024, "L1 Cache (32KB)"),
            (24 * 1024, "L1 Cache (192KB)"),
            (512 * 1024, "L2 Cache (4MB)"),
            (1024 * 1024, "L2/L3 Mix (8MB)")
        ]
        
        for size_entries, desc in test_sizes:
            count = 10_000_000
            offsets = np.random.randint(0, size_entries, count, dtype=np.uint64)
            values = np.random.randint(1, 1000000, count, dtype=np.uint64)
            
            start = time.perf_counter_ns()
            
            lib.fast_random_update(
                book_ptr,
                offsets.ctypes.data,
                values.ctypes.data,
                count
            )
            
            end = time.perf_counter_ns()
            
            rand_ns = (end - start) / count
            print(f"\n  {desc}:")
            print(f"    Per update: {rand_ns:.3f}ns")
            print(f"    Throughput: {1e9/rand_ns:,.0f} updates/sec")
        
        # Summary
        print("\n\nüéØ Performance Summary:")
        print("=" * 60)
        print(f"  Sequential (best): {seq_ns:.3f}ns")
        print(f"  Hot path: {hot_ns:.3f}ns")  
        print(f"  Random L1: ~1ns")
        print(f"  Random L2: ~10ns")
        print(f"\n‚úÖ For HFT: Use hot paths and sequential access patterns!")
        
        return hot_ns

if __name__ == "__main__":
    result = run_optimized_benchmarks()
    
    if result and result < 10:
        print(f"\nüöÄ ULTRA-FAST ACHIEVED: {result:.3f}ns per update")
        print(f"üí∞ That's {1e9/result/1e6:.0f} MILLION updates per second!")
